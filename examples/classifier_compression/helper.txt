usage: compress_classifier.py [-h] [--dataset DATASET] [--transfer]
                              [--arch ARCH] [-j N] [--epochs N] [-b N]
                              [--lr LR] [--momentum M] [--weight-decay W]
                              [--print-freq N] [--verbose]
                              [--resume-from PATH | --exp-load-weights-from PATH]
                              [--pretrained] [--reset-optimizer] [-e]
                              [--activation-stats PHASE [PHASE ...]]
                              [--activation-histograms PORTION_OF_TEST_SET]
                              [--masks-sparsity] [--param-hist]
                              [--summary {sparsity,compute,model,modules,png,png_w_params}]
                              [--export-onnx [EXPORT_ONNX]]
                              [--compress [COMPRESS]]
                              [--sense {element,filter,channel}]
                              [--sense-range SENSITIVITY_RANGE SENSITIVITY_RANGE SENSITIVITY_RANGE]
                              [--extras EXTRAS] [--deterministic]
                              [--seed SEED] [--gpus DEV_ID] [--cpu]
                              [--name NAME] [--out-dir OUTPUT_DIR]
                              [--validation-split VALIDATION_SPLIT]
                              [--effective-train-size EFFECTIVE_TRAIN_SIZE]
                              [--effective-valid-size EFFECTIVE_VALID_SIZE]
                              [--effective-test-size EFFECTIVE_TEST_SIZE]
                              [--confusion]
                              [--num-best-scores NUM_BEST_SCORES]
                              [--load-serialized] [--thinnify]
                              [--quantize-eval | --qe-calibration PORTION_OF_TEST_SET]
                              [--qe-mode QE_MODE] [--qe-bits-acts NUM_BITS]
                              [--qe-bits-wts NUM_BITS]
                              [--qe-bits-accum NUM_BITS]
                              [--qe-clip-acts QE_CLIP_ACTS]
                              [--qe-clip-n-stds QE_CLIP_N_STDS]
                              [--qe-no-clip-layers LAYER_NAME [LAYER_NAME ...]]
                              [--qe-per-channel]
                              [--qe-scale-approx-bits NUM_BITS]
                              [--qe-stats-file PATH] [--qe-config-file PATH]
                              [--save-untrained-model]
                              [--earlyexit_lossweights [EARLYEXIT_LOSSWEIGHTS [EARLYEXIT_LOSSWEIGHTS ...]]]
                              [--earlyexit_thresholds [EARLYEXIT_THRESHOLDS [EARLYEXIT_THRESHOLDS ...]]]
                              [--kd-teacher ARCH] [--kd-pretrained]
                              [--kd-resume PATH] [--kd-temperature TEMP]
                              [--kd-distill-wt WEIGHT]
                              [--kd-student-wt WEIGHT]
                              [--kd-teacher-wt WEIGHT]
                              [--kd-start-epoch EPOCH_NUM] [--greedy]
                              [--greedy-ft-epochs GREEDY_FT_EPOCHS]
                              [--greedy-target-density GREEDY_TARGET_DENSITY]
                              [--greedy-pruning-step GREEDY_PRUNING_STEP]
                              [--greedy-finetuning-policy {constant,linear-grow}]
                              DIR

Distiller image classification model compression

positional arguments:
  DIR                   path to dataset

optional arguments:
  -h, --help            show this help message and exit
  --dataset DATASET     dataset to use imagenet, cifar10, cifar10_resize,
                        mnist
  --transfer            transfer to dataset
  --arch ARCH, -a ARCH  model architecture: alexnet | alexnet_bn | bninception
                        | cafferesnet101 | densenet121 | densenet161 |
                        densenet169 | densenet201 | dpn107 | dpn131 | dpn68 |
                        dpn68b | dpn92 | dpn98 | fbresnet152 | googlenet |
                        inception_v3 | inceptionresnetv2 | inceptionv3 |
                        inceptionv4 | mobilenet | mobilenet_025 |
                        mobilenet_050 | mobilenet_075 | mobilenet_v1_dropout |
                        mobilenet_v2 | nasnetalarge | nasnetamobile |
                        plain20_cifar | pnasnet5large | polynet |
                        preact_resnet101 | preact_resnet110_cifar |
                        preact_resnet110_cifar_conv_ds | preact_resnet152 |
                        preact_resnet18 | preact_resnet20_cifar |
                        preact_resnet20_cifar_conv_ds | preact_resnet32_cifar
                        | preact_resnet32_cifar_conv_ds | preact_resnet34 |
                        preact_resnet44_cifar | preact_resnet44_cifar_conv_ds
                        | preact_resnet50 | preact_resnet56_cifar |
                        preact_resnet56_cifar_conv_ds | resnet101 |
                        resnet110_cifar_earlyexit | resnet1202_cifar_earlyexit
                        | resnet152 | resnet18 | resnet20_cifar |
                        resnet20_cifar_earlyexit | resnet32_cifar |
                        resnet32_cifar_earlyexit | resnet34 | resnet44_cifar |
                        resnet44_cifar_earlyexit | resnet50 |
                        resnet50_earlyexit | resnet56_cifar |
                        resnet56_cifar_earlyexit | resnext101_32x4d |
                        resnext101_32x8d | resnext101_64x4d | resnext50_32x4d
                        | se_resnet101 | se_resnet152 | se_resnet50 |
                        se_resnext101_32x4d | se_resnext50_32x4d | senet154 |
                        shufflenet_v2_x0_5 | shufflenet_v2_x1_0 |
                        shufflenet_v2_x1_5 | shufflenet_v2_x2_0 |
                        simplenet_cifar | simplenet_mnist | simplenet_v2_mnist
                        | squeezenet1_0 | squeezenet1_1 | vgg11 | vgg11_bn |
                        vgg11_bn_cifar | vgg11_cifar | vgg13 | vgg13_bn |
                        vgg13_bn_cifar | vgg13_cifar | vgg16 | vgg16_bn |
                        vgg16_bn_cifar | vgg16_cifar | vgg19 | vgg19_bn |
                        vgg19_bn_cifar | vgg19_cifar | xception (default:
                        resnet18)
  -j N, --workers N     number of data loading workers (default: 4)
  --epochs N            number of total epochs to run (default: 90
  -b N, --batch-size N  mini-batch size (default: 256)
  --print-freq N, -p N  print frequency (default: 10)
  --verbose, -v         Emit debug log messages
  -e, --evaluate        evaluate model on test set
  --activation-stats PHASE [PHASE ...], --act-stats PHASE [PHASE ...]
                        collect activation statistics on phases: train, valid,
                        and/or test (WARNING: this slows down training)
  --activation-histograms PORTION_OF_TEST_SET, --act-hist PORTION_OF_TEST_SET
                        Run the model in evaluation mode on the specified
                        portion of the test dataset and generate activation
                        histograms. NOTE: This slows down evaluation
                        significantly
  --masks-sparsity      print masks sparsity table at end of each epoch
  --param-hist          log the parameter tensors histograms to file (WARNING:
                        this can use significant disk space)
  --summary {sparsity,compute,model,modules,png,png_w_params}
                        sparsityprint a summary of the model, and exit -
                        options: | computeprint a summary of the model, and
                        exit - options: | modelprint a summary of the model,
                        and exit - options: | modulesprint a summary of the
                        model, and exit - options: | pngprint a summary of the
                        model, and exit - options: | png_w_params
  --export-onnx [EXPORT_ONNX]
                        export model to ONNX format
  --compress [COMPRESS]
                        configuration file for pruning the model (default is
                        to use hard-coded schedule)
  --sense {element,filter,channel}
                        test the sensitivity of layers to pruning
  --sense-range SENSITIVITY_RANGE SENSITIVITY_RANGE SENSITIVITY_RANGE
                        an optional parameter for sensitivity testing
                        providing the range of sparsities to test. This is
                        equivalent to creating sensitivities =
                        np.arange(start, stop, step)
  --extras EXTRAS       file with extra configuration information
  --deterministic, --det
                        Ensure deterministic execution for re-producible
                        results.
  --seed SEED           seed the PRNG for CPU, CUDA, numpy, and Python
  --gpus DEV_ID         Comma-separated list of GPU device IDs to be used
                        (default is to use all available devices)
  --cpu                 Use CPU only. Flag not set => uses GPUs according to
                        the --gpus flag value.Flag set => overrides the --gpus
                        flag
  --name NAME, -n NAME  Experiment name
  --out-dir OUTPUT_DIR, -o OUTPUT_DIR
                        Path to dump logs and checkpoints
  --validation-split VALIDATION_SPLIT, --valid-size VALIDATION_SPLIT, --vs VALIDATION_SPLIT
                        Portion of training dataset to set aside for
                        validation
  --effective-train-size EFFECTIVE_TRAIN_SIZE, --etrs EFFECTIVE_TRAIN_SIZE
                        Portion of training dataset to be used in each epoch.
                        NOTE: If --validation-split is set, then the value of
                        this argument is applied AFTER the train-validation
                        split according to that argument
  --effective-valid-size EFFECTIVE_VALID_SIZE, --evs EFFECTIVE_VALID_SIZE
                        Portion of validation dataset to be used in each
                        epoch. NOTE: If --validation-split is set, then the
                        value of this argument is applied AFTER the train-
                        validation split according to that argument
  --effective-test-size EFFECTIVE_TEST_SIZE, --etes EFFECTIVE_TEST_SIZE
                        Portion of test dataset to be used in each epoch
  --confusion           Display the confusion matrix
  --num-best-scores NUM_BEST_SCORES
                        number of best scores to track and report (default: 1)
  --load-serialized     Load a model without DataParallel wrapping it
  --thinnify            physically remove zero-filters and create a smaller
                        model
  --save-untrained-model
                        Save the randomly-initialized model before training
                        (useful for lottery-ticket method)
  --earlyexit_lossweights [EARLYEXIT_LOSSWEIGHTS [EARLYEXIT_LOSSWEIGHTS ...]]
                        List of loss weights for early exits (e.g.
                        --earlyexit_lossweights 0.1 0.3)
  --earlyexit_thresholds [EARLYEXIT_THRESHOLDS [EARLYEXIT_THRESHOLDS ...]]
                        List of EarlyExit thresholds (e.g.
                        --earlyexit_thresholds 1.2 0.9)
  --greedy              greedy filter pruning

Optimizer arguments:
  --lr LR, --learning-rate LR
                        initial learning rate
  --momentum M          momentum
  --weight-decay W, --wd W
                        weight decay (default: 1e-4)

Resuming arguments:
  --resume-from PATH    path to latest checkpoint. Use to resume paused
                        training session.
  --exp-load-weights-from PATH
                        path to checkpoint to load weights from (excluding
                        other fields) (experimental)
  --pretrained          use pre-trained model
  --reset-optimizer     Flag to override optimizer if resumed from checkpoint.
                        This will reset epochs count.

Arguments controlling quantization at evaluation time ("post-training quantization"):
  --quantize-eval, --qe
                        Apply linear quantization to model before evaluation.
                        Applicable only if --evaluate is also set
  --qe-calibration PORTION_OF_TEST_SET
                        Run the model in evaluation mode on the specified
                        portion of the test dataset and collect statistics.
                        Ignores all other 'qe--*' arguments
  --qe-mode QE_MODE, --qem QE_MODE
                        Linear quantization mode. Choices: sym | asym_s |
                        asym_u
  --qe-bits-acts NUM_BITS, --qeba NUM_BITS
                        Number of bits for quantization of activations
  --qe-bits-wts NUM_BITS, --qebw NUM_BITS
                        Number of bits for quantization of weights
  --qe-bits-accum NUM_BITS
                        Number of bits for quantization of the accumulator
  --qe-clip-acts QE_CLIP_ACTS, --qeca QE_CLIP_ACTS
                        Activations clipping mode. Choices: none | avg | n_std
                        | gauss | laplace
  --qe-clip-n-stds QE_CLIP_N_STDS
                        When qe-clip-acts is set to 'n_std', this is the
                        number of standard deviations to use
  --qe-no-clip-layers LAYER_NAME [LAYER_NAME ...], --qencl LAYER_NAME [LAYER_NAME ...]
                        List of layer names for which not to clip activations.
                        Applicable only if --qe-clip-acts is not 'none'
  --qe-per-channel, --qepc
                        Enable per-channel quantization of weights (per output
                        channel)
  --qe-scale-approx-bits NUM_BITS, --qesab NUM_BITS
                        Enables scale factor approximation using integer
                        multiply + bit shift, using this number of bits the
                        integer multiplier
  --qe-stats-file PATH  Path to YAML file with calibration stats. If not
                        given, dynamic quantization will be run (Note that not
                        all layer types are supported for dynamic
                        quantization)
  --qe-config-file PATH
                        Path to YAML file containing configuration for
                        PostTrainLinearQuantizer (if present, all other --qe*
                        arguments are ignored)

Knowledge Distillation Training Arguments:
  --kd-teacher ARCH     Model architecture for teacher model
  --kd-pretrained       Use pre-trained model for teacher
  --kd-resume PATH      Path to checkpoint from which to load teacher weights
  --kd-temperature TEMP, --kd-temp TEMP
                        Knowledge distillation softmax temperature
  --kd-distill-wt WEIGHT, --kd-dw WEIGHT
                        Weight for distillation loss (student vs. teacher soft
                        targets)
  --kd-student-wt WEIGHT, --kd-sw WEIGHT
                        Weight for student vs. labels loss
  --kd-teacher-wt WEIGHT, --kd-tw WEIGHT
                        Weight for teacher vs. labels loss
  --kd-start-epoch EPOCH_NUM
                        Epoch from which to enable distillation

Greedy Pruning:
  --greedy-ft-epochs GREEDY_FT_EPOCHS
                        number of epochs to fine-tune each discovered network
  --greedy-target-density GREEDY_TARGET_DENSITY
                        target density of the network we are seeking
  --greedy-pruning-step GREEDY_PRUNING_STEP
                        size of each pruning step (as a fraction in [0..1])
  --greedy-finetuning-policy {constant,linear-grow}
                        policy used for determining how long to fine-tune
